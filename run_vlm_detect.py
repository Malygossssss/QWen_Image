import os
import json
import torch
from PIL import Image
import numpy as np
import cv2
from transformers import AutoProcessor, AutoModelForImageTextToText
import re


VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

_CLASS_PATTERN = "|".join(re.escape(cls) for cls in VOC_CLASSES)
_RECOVER_PATTERN = re.compile(
    r"\{\s*\"bbox_2d\"\s*:\s*\[\s*-?\d+\s*,\s*-?\d+\s*,\s*-?\d+\s*,\s*-?\d+\s*\]\s*,\s*"
    r"\"label\"\s*:\s*\"(?:" + _CLASS_PATTERN + r")\"\s*\}"
)


def recover_partial_detections(raw_text: str):
    """ä»å¯èƒ½è¢«æˆªæ–­çš„ JSON ç‰‡æ®µä¸­æ¢å¤æ£€æµ‹ç»“æœã€‚"""
    recovered = []
    for match in _RECOVER_PATTERN.finditer(raw_text):
        try:
            recovered.append(json.loads(match.group(0)))
        except json.JSONDecodeError:
            continue
    return recovered if recovered else None


def convert_relative_to_absolute(detections, image_width, image_height):
    """
    å°†æ¨¡å‹è¾“å‡ºçš„ç›¸å¯¹åæ ‡ï¼ˆ0-1000èŒƒå›´ï¼‰è½¬æ¢ä¸ºå®é™…åƒç´ åæ ‡
    detections: [{"bbox_2d": [x1, y1, x2, y2], "label": "xxx"}, ...]
    """
    converted = []
    for det in detections:
        rel_x1, rel_y1, rel_x2, rel_y2 = det["bbox_2d"]
        # å°†ç›¸å¯¹åæ ‡è½¬æ¢ä¸ºç»å¯¹åæ ‡
        abs_x1 = int(rel_x1 / 1000.0 * image_width)
        abs_y1 = int(rel_y1 / 1000.0 * image_height)
        abs_x2 = int(rel_x2 / 1000.0 * image_width)
        abs_y2 = int(rel_y2 / 1000.0 * image_height)
        
        converted.append({
            "bbox_2d": [abs_x1, abs_y1, abs_x2, abs_y2],
            "label": det["label"]
        })
    return converted


def draw_bboxes(image, detections):
    """
    åœ¨è¾“å…¥ PIL.Image ä¸Šç”»å‡ºç›®æ ‡æ£€æµ‹æ¡†å’Œæ ‡ç­¾
    detections: [{"bbox_2d": [x1, y1, x2, y2], "label": "xxx"}, ...]
    """
    img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    for det in detections:
        x1, y1, x2, y2 = map(int, det["bbox_2d"])  # åæ ‡å–æ•´
        label = det["label"]
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(img, label, (x1, max(20, y1 - 5)),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
    return Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))


# ==============================
# Step 1: åŠ è½½æ¨¡å‹
# ==============================
MODEL_PATH = "/nas_pub_data/models/base/qwen3-vl-4b-instruct"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
OUTPUT_ROOT = "outputs"
os.makedirs(OUTPUT_ROOT, exist_ok=True)

print(f"Loading model from {MODEL_PATH} on {DEVICE}...")

processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForImageTextToText.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    dtype=torch.bfloat16,
    trust_remote_code=True
).eval()


# ==============================
# Step 2: éå†ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡
# ==============================
IMAGE_DIR = "/mnt/pub/wyf/workspace/image_identification/Pascal-VOC-2012/valid/images"
image_files = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith((".jpg", ".png", ".jpeg"))]

for img_file in image_files:
    IMAGE_PATH = os.path.join(IMAGE_DIR, img_file)
    image = Image.open(IMAGE_PATH).convert("RGB")

    # ==============================
    # Step 3: æ„é€  Prompt
    # ==============================
    conversation = [
        {"role": "user", "content": [
            {"type": "image"},
            {"type": "text", "text": (
                "Locate every instance that belongs to the following categories: 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', "
                "'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'. "
                "Report bbox coordinates in JSON format."
            )}
        ]}
    ]
    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)

    # ==============================
    # Step 4: æ¨¡å‹æ¨ç†
    # ==============================
    inputs = processor(text=prompt, images=image, return_tensors="pt").to(DEVICE)
    output = model.generate(**inputs, max_new_tokens=512)

    result_text = processor.batch_decode(output, skip_special_tokens=True)[0]
    print(f"\nğŸ“· {img_file} Raw model output:\n", result_text)

    # ä»…ä¿ç•™æœ€åä¸€æ¬¡ assistant çš„å›ç­”å†…å®¹ï¼Œé¿å… prompt å¹²æ‰° JSON è§£æ
    assistant_marker = "\nassistant\n"
    marker_idx = result_text.rfind(assistant_marker)
    if marker_idx != -1:
        assistant_text = result_text[marker_idx + len(assistant_marker):].strip()
    else:
        # æœ‰äº›æ¨¡å‹ä½¿ç”¨ <|im_start|>assistant ä½œä¸ºåˆ†éš”ç¬¦
        alt_marker = "<|im_start|>assistant"
        alt_idx = result_text.rfind(alt_marker)
        if alt_idx != -1:
            assistant_text = result_text[alt_idx + len(alt_marker):].strip()
        else:
            assistant_text = result_text

    # ==============================
    # Step 5: åˆ›å»ºè¾“å‡ºç›®å½•
    # ==============================
    img_name = os.path.splitext(img_file)[0]
    save_dir = os.path.join(OUTPUT_ROOT, img_name)
    os.makedirs(save_dir, exist_ok=True)

    with open(os.path.join(save_dir, "result.txt"), "w", encoding="utf-8") as f:
        f.write(result_text)

    # ==============================
    # Step 6: å°è¯•è§£æ JSON å¹¶ä¿å­˜
    # ==============================
    detections = None
    parse_failed = False
    try:
        # â‘  æå– ```json ... ``` ä¸­çš„ JSON å†…å®¹
        match = re.search(r"```(?:json)?\s*(\[[\s\S]*?\])\s*```", assistant_text)
        if match:
            json_str = match.group(1).strip()
            detections = json.loads(json_str)
        else:
            # å¦‚æœæ²¡æœ‰ä»£ç å—æ ‡è®°ï¼Œåˆ™é€€è€Œæ±‚å…¶æ¬¡æŸ¥æ‰¾ç¬¬ä¸€ä¸ª [ ] æ®µ
            start = assistant_text.find("[")
            end = assistant_text.rfind("]")
            if start != -1 and end != -1:
                json_str = assistant_text[start:end+1]
                detections = json.loads(json_str)
            else:
                parse_failed = True
                print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆ JSON æ–¹æ‹¬å·")

        # å°†å­—ç¬¦ä¸²æˆ–å­—å…¸å½¢å¼çš„ç»“æœè½¬æ¢ä¸ºåˆ—è¡¨ï¼Œä¾¿äºåç»­å¤„ç†
        if isinstance(detections, str):
            detections = json.loads(detections)
        if isinstance(detections, dict):
            detections = [detections]

    except Exception as e:
        parse_failed = True
        print("âš ï¸ JSON è§£æå¤±è´¥:", e)
        print("ğŸ” åŸå§‹è¾“å‡ºç‰‡æ®µ:", assistant_text[:300])

    if detections is None:
        recovered = recover_partial_detections(assistant_text)
        if recovered:
            detections = recovered
            if parse_failed:
                print(f"âš ï¸ å·²ä»éƒ¨åˆ† JSON ä¸­æ¢å¤ {len(detections)} ä¸ªæ£€æµ‹ç»“æœ")
            else:
                print(f"âš ï¸ å·²ä»æ–‡æœ¬ä¸­æå– {len(detections)} ä¸ªæ£€æµ‹ç»“æœ")
        elif parse_failed:
            print("âš ï¸ æ— æ³•ä»æ¨¡å‹è¾“å‡ºä¸­æ¢å¤æœ‰æ•ˆæ£€æµ‹ç»“æœ")

    # ==============================
    # Step 7: ä¿å­˜ç»“æœä¸å¯è§†åŒ–
    # ==============================
    if detections is None:
        print(f"âŒ {img_file} æœªæ£€æµ‹åˆ°è¾¹ç•Œæ¡†ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è¾“å‡º")
        continue

    # åœ¨ä¿å­˜å‰ï¼Œåˆ†åˆ«è®°å½•ç›¸å¯¹åæ ‡ï¼ˆ0-1000ï¼‰ä¸è½¬æ¢åçš„ç»å¯¹åƒç´ åæ ‡
    with open(os.path.join(save_dir, "result.json"), "w", encoding="utf-8") as f:
        json.dump(detections, f, ensure_ascii=False, indent=2)

    image_width, image_height = image.size
    detections_absolute = convert_relative_to_absolute(
        detections, image_width, image_height
    )

    with open(os.path.join(save_dir, "result_absolute.json"), "w", encoding="utf-8") as f:
        json.dump(detections_absolute, f, ensure_ascii=False, indent=2)

    out_img = os.path.join(save_dir, "detection.jpg")
    vis_img = draw_bboxes(image, detections_absolute)
    vis_img.save(out_img)

    if detections:
        print(f"âœ… æ£€æµ‹ç»“æœå·²ä¿å­˜åˆ° {save_dir}")
    else:
        print(f"âš ï¸ {img_file} æœªè§£æåˆ°æ£€æµ‹ç»“æœï¼Œå·²ä¿å­˜ç©ºç»“æœæ–‡ä»¶")